## Offline
- construct dataset with logged propensities
- construct bandit dataset (done=True)
- implement IW versions of algorithms
- make discrete action space version
- check that bandit Q learning will work as expected



## Online
- try running more policy/Q updates per step
- tune hyperparameters (n_samples, quantile, target_entropy) 
- run more seeds