## Offline
- add weight saving
- add propensity logging
- construct dataset with logged propensities
- construct bandit dataset (done=True)
- check that bandit Q learning will work as expected
- implement IW versions of algorithms



## Online
- optimize all sampled actions
- try running more policy/Q updates per step
- tune hyperparameters (n_samples, quantile, target_entropy) 
- run more seeds